# -*- coding: utf-8 -*-
"""Text_mining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f89csCS8r3eM_ayQsCk4HRM5NIB1RMkH
"""

text1 = 'ethics are built right into the ideals and objectives of United'

"""# Basic Operations"""

len(text1)

text2 = text1.split(' ')
#.splitlines()
#.join(t)  opposite of splitting

len(text2)

text2

# to find text that is >3 letter long
[w for w in text2 if len(w) > 3 ]

# to check if word starts with capital letter
[w for w in text2 if w.istitle()]
#isupper()
#islower()
#isalpha
#isdigit
#isalnum

#to check if the word ends with 's'
[w for w in text2 if w.endswith('s')]

#startswith()
#t in s

text3 = 'To be or not to be'

text4 = text3.split(' ')

len(set(text4)) # but only 4 unique to be or not

len(set([w.lower() for w in text4]))
#.upper()
#.titlecase()

'''
s.strip() it will remove all the white space characters from the string from the front and end
s.rstrip() from the end of the string
s.find(t) will give the index
s.rfind(t) index from reverse
s.replace(u,v) replace every occurence with v
'''

t = 'ouagadouug' # it wont get split because there is not separator so we can use list(t)
# or [c for c in t]

#cleaning
text8 = '    A quick brown frog jumped over the body.  '
# if split by ' ' then empty string will also come
text9 =text8.strip()

text9.split(' ')



"""# Larger Files"""

'''
f = open('file.txt', 'r') in read mode
f.readline() read the first line, it will give \n at the end


f.seek(0) reset the reading to 0 
text12 = f.read() or f.read(n) to read n characters
f.write(message)
f.close()
text13 = text12.splitlines() delimited by backslash \n
'''

"""# Regex"""

'''We can use regular expressions to help us with more complex parsing.

For example '@[A-Za-z0-9_]+' will return all words that:

start with '@' and are followed by at least one:
capital letter ('A-Z')
lowercase letter ('a-z')
number ('0-9')
or underscore ('_')
'''

#import re # import re - a module that provides support for regular expressions

#[w for w in text8 if re.search('@[A-Za-z0-9_]+', w)]

# + means for one or more times

'''
.  = wildcard, matches a single character
^ = start of a string
$ = end of the string
[] = mathces one of the set of characters within []
[a-z] = matches one of the range of characters
[^abc] = not a or b or c
a|b = matches a or b, where a and b are strings
\d = any digit
\D = any non digit
\s = any whitespace character
\S = any non white space character
\w = any alphanumeric characteer
\W = .......


* = 0 or more times
+ = one or more times
? = 0 or one
{3} = exactly 3
{n,} = atleast n
{,n} = atmost n
{m,n} = atleast m and atmost n

'''

# [w for w in text12 if re.search('@\w+',text12)] it will gives words starting with @ with one or more occurences of alphnumeric

#re.findall(r'[aeiou]', text12) # find all vowels

# to write date
# exp =re.findall( r'\d{2}[\-]\d{2}[\-]\d{4}', text12)
# r is used to tell that take whole as a raw string

# r'\d{2} (?:jan|feb|march) \d{4}'
'''
two digits with a slash or a dash then again for month and then 4 digit for year
 '''



import pandas as pd

time_sentences = ["Monday: The doctor's appointment is at 2:45pm.", 
                  "Tuesday: The dentist's appointment is at 11:30 am.",
                  "Wednesday: At 7:00pm, there is a basketball game!",
                  "Thursday: Be back home by 11:15 pm at the latest.",
                  "Friday: Take the train at 08:10 am, arrive at 09:00am."]

df = pd.DataFrame(time_sentences, columns=['text'])
df

# find the number of characters for each string in df['text']
df['text'].str.len()

# find the number of tokens for each string in df['text']
df['text'].str.split().str.len()

# find which entries contain the word 'appointment'
df['text'].str.contains('appointment')

# find how many times a digit occurs in each string
df['text'].str.count(r'\d')

# find all occurances of the digits
df['text'].str.findall(r'\d')

# group and find the hours and minutes
df['text'].str.findall(r'(\d?\d):(\d\d)')

# create new columns from first match of extracted groups
df['text'].str.extract(r'(\d?\d):(\d\d)')

# extract the entire time, the hours, the minutes, and the period
df['text'].str.extractall(r'((\d?\d):(\d\d) ?([ap]m))')

# extract the entire time, the hours, the minutes, and the period with group names
df['text'].str.extractall(r'(?P<time>(?P<hour>\d?\d):(?P<minute>\d\d) ?(?P<period>[ap]m))')



"""# Assignment 1

Extract all the dates
"""

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

import pandas as pd

doc = []
with open('dates.txt') as file:
    for line in file:
        doc.append(line)

df = pd.Series(doc)
df.head(10)

def date_sorter():
    
    regex1 = '(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})'
    regex2 = '((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[\S]*[+\s]\d{1,2}[,]{0,1}[+\s]\d{4})'
    regex3 = '(\d{1,2}[+\s](?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[\S]*[+\s]\d{4})'
    regex4 = '((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[\S]*[+\s]\d{4})'
    regex5 = '(\d{1,2}[/-][1|2]\d{3})'
    regex6 = '([1|2]\d{3})'
    full_regex = '(%s|%s|%s|%s|%s|%s)' %(regex1, regex2, regex3, regex4, regex5, regex6)
    parsed_date = df.str.extract(full_regex)
    parsed_date = parsed_date.iloc[:,0].str.replace('Janaury', 'January').str.replace('Decemeber', 'December')
    parsed_date = pd.Series(pd.to_datetime(parsed_date))
    parsed_date = parsed_date.sort_values(ascending=True).index
    return pd.Series(parsed_date.values)



"""# NLTK Basic"""

import nltk

nltk.download()

nltk.download('gutenberg')
nltk.download('genesis')
nltk.download('treebank')
nltk.download('nps_chat')
nltk.download('webtext')
nltk.download('inaugural')
#nltk.download('all')

from nltk.book import *

text1

sents() # one sentence each from each text corpora

sent1 # first sentence from text1 into list form

sent7

len(sent7) # may not be unique

len(text7) # total words may not be unique

len(set(text7))

#list(text7) # all the words in the text7

list(set(text7))[:10]

"""Freq of words"""

# frequency of words
d = FreqDist(text7)

len(d)

vocab_words = list(d.keys())

vocab_words[:10]

d[',']

freq_words = [w for w in vocab_words if len(w) > 5 and d[w] > 100]

freq_words

"""**Normalising and stemming**

*  Normalisation is when we transform the words to make it appear the same way say play, played, Play, playing etc
*   Stemming is to find the root word
"""

t = 'play played Play plays playing playings'

word = t.lower().split(' ')

word

porter = nltk.PorterStemmer()

[porter.stem(w) for w in word]

"""Lemmatisation"""

nltk.download('udhr')
udhr = nltk.corpus.udhr.words('English-Latin1')
# universal declaration of human rights

udhr[:20]

[porter.stem(w) for w in udhr[:20]]  # here we can say that some words are not valid so lemmatization

nltk.download('wordnet')
WNlemma = nltk.WordNetLemmatizer()
[WNlemma.lemmatize(t) for t in udhr[:20]]

"""Tokenisation"""

nltk.download('punkt')
text11 = "Children shouldn't drink a sugary drink before bed."
text11.split(' ') # it is also keeping full stop so we can use inbuilt tokeniser

nltk.word_tokenize(text11)

text12 = "This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!"
sentences = nltk.sent_tokenize(text12) # nltk inbuilt sentence splitter
len(sentences)

sentences



"""# NLTK Advanced

POS(Part of speech) tagging
"""

import nltk
nltk.download('tagsets')

nltk.help.upenn_tagset('MD')

text11 = "Children shouldn't drink a sugary drink before bed."

text13 = nltk.word_tokenize(text11)

nltk.download('averaged_perceptron_tagger')
nltk.pos_tag(text13)

nltk.download('punkt')
# Parsing sentence structure
text15 = nltk.word_tokenize("Alice loves Bob")
grammar = nltk.CFG.fromstring("""
S -> NP VP
VP -> V NP
NP -> 'Alice' | 'Bob'
V -> 'loves'
""")

parser = nltk.ChartParser(grammar)
trees = parser.parse_all(text15)
for tree in trees:
    print(tree)

# generating grammer rules is very time consuming task
nltk.download('treebank')
from nltk.corpus import treebank
text17 = treebank.parsed_sents('wsj_0001.mrg')[0]
print(text17)



"""# Assignment 2"""

# ratio of whale and Whale count by total freq of words
'''
def answer_two():
    
    dist = nltk.FreqDist(text1)
    return (dist['whale'] + dist['Whale'])/len(text1)*100

answer_two()


'''
# most common 20 words
#return nltk.FreqDist(text1).most_common(20)

'''
#length of greater than 5 and frequency of more than 150?
    dist = nltk.FreqDist(text1)
    vocab1 = dist.keys()
    freqwords = [w for w in vocab1 if len(w)>5 and dist[w] > 150]
    return sorted(freqwords)
'''

'''
#longest word in text1 and that word's length.

    maxlen = max(len(w) for w in text1)
    word = [w for w in text1 if len(w) == maxlen]
    return (word[0],maxlen)
'''

#unique words have a frequency of more than 2000? What is their frequency?
'''
    dist = nltk.FreqDist(text1)
    freqwords = [(dist[w],w) for w in set(text1) if w.isalpha() and dist[w]>2000]
    return sorted(freqwords, key=lambda x:x[0],reverse=True)

'''

#What are the 5 most frequent parts of speech in this text? What is their frequency?
'''
    import collections
    pos_list = nltk.pos_tag(text1)
    pos_counts = collections.Counter((subl[1] for subl in pos_list))
    return pos_counts.most_common(5)
'''

#spelling recommendation

'''

from nltk.corpus import words
correct_spellings = words.words()

#For every misspelled word, the recommender should find find the word in correct_spellings that has the shortest distance*, 
#and starts with the same letter as the misspelled word, and return that word as a recommendation.

#*Each of the three different recommenders will use a different distance measure


def answer(entries=['cormulent', 'incendenece', 'validrate']):
    
    recommend = []
    for entry in entries:
        
        # Match first letter. input_spell contains all words in correct_spellings with the same first letter.
        input_spell = [x for x in correct_spellings if x[0] == entry[0]]
        
        # Find the jaccard distance between the entry word and every word in correct_spellings with the same first letter.
        jaccard_dist = [nltk.jaccard_distance(set(nltk.ngrams(entry,n=3)), set(nltk.ngrams(x,n=3))) for x in input_spell]
        
        # Recommend the word in input_spell with the minimum Jaccard distance.
        recommend.append(input_spell[np.argmin(jaccard_dist)])
        
    return recommend

'''
'''

    recommend = []
    for entry in entries:
        
        # Match first letter. input_spell contains all words in correct_spellings with the same first letter.
        input_spell = [x for x in correct_spellings if x[0] == entry[0]]
        
        # Find the edit between the entry word and every word in correct_spellings with the same first letter.
        DL_dist = [nltk.edit_distance(x, entry, transpositions=True) for x in input_spell]
        
        # Recommend the word in input_spell with the minimum edit.
        recommend.append(input_spell[np.argmin(DL_dist)])
        
    return recommend
'''



"""# Classification"""

'''
from nltk.classify import NaiveBayesClassifier
classifier = NaiveBayesClassifier.train(train_data)
'''

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

'''

import pandas as pd
import numpy as np

# Read in the data
df = pd.read_csv('Amazon_Unlocked_Mobile.csv')

# Sample the data to speed up computation
# Comment out this line to match with lecture
df = df.sample(frac=0.1, random_state=10)

df.head()

#=========================================
#preprocessing

# Drop missing values
df.dropna(inplace=True)

# Remove any 'neutral' ratings equal to 3
df = df[df['Rating'] != 3]

# Encode 4s and 5s as 1 (rated positively)
# Encode 1s and 2s as 0 (rated poorly)
df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)
df.head(10)
'''

'''
# Most ratings are positive
df['Positively Rated'].mean()
'''

'''
from sklearn.model_selection import train_test_split

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], 
                                                    df['Positively Rated'], 
                                                    random_state=0)
'''

'''
print('X_train first entry:\n\n', X_train.iloc[0])
print('\n\nX_train shape: ', X_train.shape)
'''

#####CountVectorizer
'''
from sklearn.feature_extraction.text import CountVectorizer

# Fit the CountVectorizer to the training data
vect = CountVectorizer().fit(X_train)
'''

'''
vect.get_feature_names()[::2000]
'''

'''
len(vect.get_feature_names())
'''

'''
# transform the documents in the training data to a document-term matrix
X_train_vectorized = vect.transform(X_train)

X_train_vectorized
'''
# each row is a document and each column is a word of the vocabulary

# training
'''
from sklearn.linear_model import LogisticRegression

# Train the model
model = LogisticRegression()
model.fit(X_train_vectorized, y_train)
'''
# Logistics regression works for high dimensional sparse data

# predict
'''
from sklearn.metrics import roc_auc_score

# Predict the transformed test documents
predictions = model.predict(vect.transform(X_test))

print('AUC: ', roc_auc_score(y_test, predictions))
'''

'''
# get the feature names as numpy array
feature_names = np.array(vect.get_feature_names())

# Sort the coefficients from the model
sorted_coef_index = model.coef_[0].argsort()

# Find the 10 smallest and 10 largest coefficients
# The 10 largest coefficients are being indexed using [:-11:-1] 
# so the list returned is in order of largest to smallest
print('Smallest Coefs:\n{}\n'.format(feature_names[sorted_coef_index[:10]]))
print('Largest Coefs: \n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))
'''

#Tfidf term frequency inverse documnet frequency
# it allows us to weight terms based on how imp they are to the document
'''
from sklearn.feature_extraction.text import TfidfVectorizer

# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5
vect = TfidfVectorizer(min_df=5).fit(X_train)
len(vect.get_feature_names())
'''
#will give same number of features as we got earlier
#min_df = 5 means the tfidf will remove those words that appear in fewer than 5 documents

# features with high weight appear often in a particular document but not so often in corpus
# features with low tfidf are either used commonly across all the documents or rarely  in long documents

'''
X_train_vectorized = vect.transform(X_train)

model = LogisticRegression()
model.fit(X_train_vectorized, y_train)

predictions = model.predict(vect.transform(X_test))

print('AUC: ', roc_auc_score(y_test, predictions))
'''

'''
feature_names = np.array(vect.get_feature_names())

sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()

print('Smallest tfidf:\n{}\n'.format(feature_names[sorted_tfidf_index[:10]]))
print('Largest tfidf: \n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))
'''

'''
sorted_coef_index = model.coef_[0].argsort()

print('Smallest Coefs:\n{}\n'.format(feature_names[sorted_coef_index[:10]]))
print('Largest Coefs: \n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))
'''

'''
# These reviews are treated the same by our current model
print(model.predict(vect.transform(['not an issue, phone is working',
                                    'an issue, phone is not working'])))
'''

#n-grams

#we can add context by adding sequences of word features

'''
# Fit the CountVectorizer to the training data specifiying a minimum 
# document frequency of 5 and extracting 1-grams and 2-grams
vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)

X_train_vectorized = vect.transform(X_train)

len(vect.get_feature_names())
'''

'''
model = LogisticRegression()
model.fit(X_train_vectorized, y_train)

predictions = model.predict(vect.transform(X_test))

print('AUC: ', roc_auc_score(y_test, predictions))'''

'''
feature_names = np.array(vect.get_feature_names())

sorted_coef_index = model.coef_[0].argsort()

print('Smallest Coefs:\n{}\n'.format(feature_names[sorted_coef_index[:10]]))
print('Largest Coefs: \n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))
'''

'''
# These reviews are now correctly identified
print(model.predict(vect.transform(['not an issue, phone is working',
                                    'an issue, phone is not working'])))
'''



"""# Text Modelling"""

import nltk
from nltk.corpus import wordnet as wn

nltk.download('wordnet')

# finding appropriate sense of words
deer = wn.synset('deer.n.01') # sysnet of deer which is a noun and gives the 1st synset

# to find path similarity
#deer.path_similarity(horse)

# to find lin similarity
nltk.download('wordnet_ic')
from nltk.corpus import wordnet_ic
brown_ic = wordnet_ic.ic('ic-brown.dat')

#deer.lin_similarity(elk, brown_ic)

from nltk.collocations import *

bigram_measures = nltk.collocations.BigramAssocMeasures()
finder = BigramCollocationFinder.from_words(text)
finder.nbest(bigram_measures.pmi, 10)
#finder.apply_freq_filter(10)

"""# Assignment 4"""

